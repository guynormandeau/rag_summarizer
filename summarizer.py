import re
from transformers import pipeline

import fitz  # PyMuPDF

from docx import Document

def extract_text_from_docx(file_path):
    """Extracts text from a DOCX file."""
    try:
        doc = Document(file_path)
        text = "\n".join([paragraph.text for paragraph in doc.paragraphs])
        return text
    except Exception as e:
        return f"Error extracting DOCX: {e}"

def extract_text_from_pdf(pdf_path):
    """Extracts text from a PDF file."""
    text = ""
    try:
        with fitz.open(pdf_path) as doc:
            for page in doc:
                text += page.get_text("text") + "\n"
    except Exception as e:
        return f"Error extracting text from PDF: {str(e)}"
    return text.strip()

# Charger le mod√®le de r√©sum√©
summarizer = pipeline("summarization", model="google/flan-t5-base")

def chunk_text(text, chunk_size=1024):
    """Divise le texte en segments de `chunk_size` mots (plus longs pour am√©liorer la coh√©rence)."""
    words = text.split()
    return [" ".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]

def clean_summary(text):
    """Corrige certaines erreurs linguistiques dans le r√©sum√©."""
    text = text.replace("and", "et").replace("of", "de")
    text = re.sub(r'\bmod√©les\b', "mod√®les", text)
    text = re.sub(r'\bgrets\b', "grands", text)  # Correction automatique
    return text.strip()

def summarize_text(text, num_paragraphs=5):
    """G√©n√®re un r√©sum√© plus d√©taill√© et mieux structur√©."""

    text = text.strip().replace("\n", " ").replace("\r", " ")  # Nettoyage du texte

    if not text:
        return "Le fichier ne contient aucun texte exploitable."

    text_length = len(text.split())
    if text_length < 100:
        return "Le texte est trop court pour √™tre r√©sum√©."

    # üîπ AUGMENTATION DE LA LONGUEUR DU R√âSUM√â
    max_length = int(min(text_length // 1.1, 1200))  # Augment√© √† 1200 tokens pour un r√©sum√© plus complet
    min_length = int(max_length // 2)  # On s'assure que ce soit d√©taill√©

    # üîπ Segmentation du texte (√©vite une d√©coupe trop fine)
    if text_length <= 1200:
        chunks = [text]
    else:
        chunks = chunk_text(text, chunk_size=1024)

    summarized_chunks = []
    for chunk in chunks:
        try:
            summary = summarizer(chunk, max_length=max_length, min_length=min_length, do_sample=False)
            summarized_chunks.append(summary[0]['summary_text'])
        except Exception as e:
            summarized_chunks.append(f"[Erreur : {str(e)}]")

    if not summarized_chunks or all("Erreur" in chunk for chunk in summarized_chunks):
        return "Erreur : Aucun r√©sum√© g√©n√©r√©."

    # üîπ AM√âLIORATION : On fusionne le r√©sum√© et ajuste mieux les paragraphes
    final_summary = " ".join(summarized_chunks)
    return split_into_paragraphs(final_summary, num_paragraphs)

def split_into_paragraphs(text, num_paragraphs):
    """Divise un r√©sum√© en plusieurs paragraphes √©quilibr√©s."""
    sentences = re.split(r'(?<=\.)\s+', text)  # S√©paration par phrases

    if len(sentences) < num_paragraphs:
        num_paragraphs = len(sentences)  # √âvite d'ajouter des paragraphes artificiels

    avg_sentences_per_paragraph = max(2, len(sentences) // num_paragraphs)

    paragraphs = []
    for i in range(0, len(sentences), avg_sentences_per_paragraph):
        paragraph = " ".join(sentences[i:i + avg_sentences_per_paragraph])
        paragraphs.append(paragraph.strip())

    return "\n\n".join(paragraphs[:num_paragraphs])  # Retourne exactement le nombre demand√©